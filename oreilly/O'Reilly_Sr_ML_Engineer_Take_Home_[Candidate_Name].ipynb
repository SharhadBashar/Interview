{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw38KAbDKSOm"
      },
      "source": [
        "# O'Reilly (Sr.) Machine Learning Engineer Takehome\n",
        "\n",
        "Welcome to the evaluation project for the (Sr.) Machine Learning Engineer position at O'Reilly Media. In this project you will evaluate a search academic dataset built using common learn-to-rank features, build a ranking model using the dataset, and discuss how additional features could be used and how they would impact the performance of the model.\n",
        "\n",
        "Overview:\n",
        "- Make a copy of this notebook\n",
        "- Fill in the contact info\n",
        "- Download the dataset to the notebook (link in Step 3.2 comments)\n",
        "- Preprocess and evaluate the dataset\n",
        "- Build a **ranking** model\n",
        "- Evaluate your ranking model using a metric of your choice\n",
        "- Answer discussion questions\n",
        "- Submit your notebook\n",
        "\n",
        "\n",
        "## Notes\n",
        "\n",
        "\n",
        "\n",
        "*   Throughout the notebook you should include notes explaining your choices and what you are doing. Your thought process is more important than the actual performance of your model.\n",
        "*   Create as many cells as you want. The exisiting cells are just provided to provide some initial organization.\n",
        "* You may use any choice of libraries or frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNewGEbOaxSp"
      },
      "source": [
        "# Step 1. Make a copy of this Colab notebook and work on your personal copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81Z26LXfZmv7"
      },
      "source": [
        "# Step 2. Fill in your contact information\n",
        "\n",
        "Candidate Full Name: Sharhad Bashar\n",
        "\n",
        "Candidate Email: sharhad.bashar@uwaterloo.ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrjg_MS-5_t"
      },
      "source": [
        "# Step 3. Create the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdtz-fNP-eq7"
      },
      "source": [
        "### 1) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vQbF5lfeQ7B-"
      },
      "outputs": [],
      "source": [
        "# Import dependencies here\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1am1Iv_LWR2W"
      },
      "source": [
        "### 2) Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_4tmowxOHAF"
      },
      "outputs": [],
      "source": [
        "# Download the dataset located at https://storage.googleapis.com/personalization-takehome/MSLR-WEB10K.zip\n",
        "# You can read about the features included in the dataset here: https://www.microsoft.com/en-us/research/project/mslr/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdKuIDNWVb8"
      },
      "source": [
        "### 3) Preprocess and evaluate the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_numeric(df):\n",
        "    for col in range(2, df.shape[1]):  # Adjust the range based on your actual data\n",
        "        df[col] = pd.to_numeric(df[col], errors = 'coerce')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WHEqbC9sOrvb"
      },
      "outputs": [],
      "source": [
        "# Preprocess and evaluate the dataset\n",
        "\n",
        "# Load the training, validation, and test data\n",
        "fold = 1\n",
        "train_data = pd.read_csv(f'MSLR-WEB10K/Fold{fold}/train.txt', sep = ' ', header = None)\n",
        "validate_data = pd.read_csv(f'MSLR-WEB10K/Fold{fold}/vali.txt', sep = ' ', header = None)\n",
        "test_data = pd.read_csv(f'MSLR-WEB10K/Fold{fold}/test.txt', sep = ' ', header = None)\n",
        "\n",
        "# Convert the datasets\n",
        "train_data = convert_to_numeric(train_data)\n",
        "validate_data = convert_to_numeric(validate_data)\n",
        "test_data = convert_to_numeric(test_data)\n",
        "\n",
        "# Extract features and labels from each dataset\n",
        "y_train = train_data[0]  # relevance labels for training\n",
        "qid_train = train_data[1]  # query IDs for training\n",
        "X_train = train_data.drop([0, 1], axis = 1)  # feature vectors for training\n",
        "\n",
        "y_validate = validate_data[0]  # relevance labels for validation\n",
        "qid_validate = validate_data[1]  # query IDs for validation\n",
        "X_validate = validate_data.drop([0, 1], axis = 1)  # feature vectors for validation\n",
        "\n",
        "y_test = test_data[0]  # relevance labels for testing\n",
        "qid_test = test_data[1]  # query IDs for testing\n",
        "X_test = test_data.drop([0, 1], axis = 1)  # feature vectors for testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OICi0aAuWclQ"
      },
      "source": [
        "### 4) Build ranking model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "frHrBKmTSUsq"
      },
      "outputs": [],
      "source": [
        "# Build ranking model\n",
        "# Prepare DMatrix for XGBoost\n",
        "train_matrix = xgb.DMatrix(X_train, label=y_train, group=qid_train.value_counts().sort_index().values)\n",
        "validate_matrix = xgb.DMatrix(X_validate, label=y_validate, group=qid_validate.value_counts().sort_index().values)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'rank:pairwise',\n",
        "    'eval_metric': 'ndcg',\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 10,\n",
        "    'min_child_weight': 300,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'verbosity': 1\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\teval-ndcg:0.31999\n",
            "[1]\teval-ndcg:0.31999\n",
            "[2]\teval-ndcg:0.31999\n",
            "[3]\teval-ndcg:0.31999\n",
            "[4]\teval-ndcg:0.31999\n",
            "[5]\teval-ndcg:0.31999\n",
            "[6]\teval-ndcg:0.31999\n",
            "[7]\teval-ndcg:0.31999\n",
            "[8]\teval-ndcg:0.31999\n",
            "[9]\teval-ndcg:0.31999\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model = xgb.train(params, train_matrix, num_boost_round=100, evals=[(validate_matrix, 'eval')], early_stopping_rounds=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBtVqWRSWx_g"
      },
      "source": [
        "### 5) Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FpaP8jesTQMj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.1696\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model performance\n",
        "# Prepare DMatrix for test set\n",
        "test_matrix = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Predict using the test data\n",
        "y_pred = model.predict(test_matrix)\n",
        "\n",
        "# Evaluate using NDCG\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "ndcg = ndcg_score([y_test], [y_pred], k=10)\n",
        "print(f\"NDCG@10: {ndcg:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'num_leaves': [31, 63],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'num_iterations': [100, 200]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lgb.LGBMRanker(), param_grid=param_grid, cv=gkf, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y, group=qid)\n",
        "print(\"Best parameters found: \", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo89zS_g--XG"
      },
      "source": [
        "# Step 4. Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f3RmmCUTdNp"
      },
      "source": [
        "### 1) Please answer the following questions about your choices:\n",
        "- Discuss your model and why you chose the model you chose (eg architecture, design, loss functions, etc)\n",
        "- Why did you choose your metric to evaluate the model?\n",
        "- How well would you say your model performed?\n",
        "- If you had more time what else would you want to try?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYwv7d8pTn6V"
      },
      "source": [
        "### 2) Please answer the following questions about how you would use additional features:\n",
        "\n",
        "- If you had an additional feature for each row of the dataset that was unique identifier for the user performing the query e.g. `user_id`, how could you use it to improve the performance of the model?\n",
        "- If you had the additional features of: `query_text` or the actual textual query itself, as well as document text features like `title_text`, `body_text`, `anchor_text`, `url` for the document, how would you include them in your model (or any model) to improve its performance?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJGp9xC2uVK3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy73cbghWBM3"
      },
      "source": [
        "# Step 5. Please share your colab notebook with: qma@oreilly.com and jtorres@oreilly.com  \n",
        "The share icon is located in the upper right corner of your Colab notebook.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

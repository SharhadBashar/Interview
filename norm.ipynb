{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Byte-Pair Encoding Tokenization\n",
        "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa. In this exercise, you will be implementing BPE based on a simple text corpus, namely the text of the Constitution of New York (1777)."
      ],
      "metadata": {
        "id": "IywT3mfixz8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Text Tokenization\n",
        "The first task is to implement a `tokenize` function that accepts a string and a vocabulary and turns the input string into a list of tokens. For example:\n",
        "\n",
        "```python\n",
        ">>> tokenize(\"hello world\", [\" wo\", \"rld\", \"he\", \"llo\"])\n",
        "['he', 'llo', ' wo', 'rld']\n",
        ">>> tokenize(\"hehehe\", [\" wo\", \"rld\", \"he\", \"llo\"])\n",
        "['he', 'he', 'he']\n",
        "```\n",
        "\n",
        "If the provided input string cannot be parsed into the provided vocabulary of tokens, you can raise an exception.\n",
        "\n",
        "**HINT**: Use `s.startswith(prefix)`"
      ],
      "metadata": {
        "id": "0tv-L-Ttx8jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(chunk: str, vocab: list[str]) -> list[str]:\n",
        "  tokens = []\n",
        "  vocab = sorted(vocab, key = len, reverse = True)\n",
        "  while chunk:\n",
        "    match = None\n",
        "    for token in vocab:\n",
        "      if (chunk.startswith(token)):\n",
        "        match = token\n",
        "        break\n",
        "    if (match is None):\n",
        "      raise ValueError(f'No tokens for {chunk}')\n",
        "    tokens.append(match)\n",
        "    chunk = chunk[len(match):]\n",
        "  return tokens\n",
        "tokenize(\"hello world\", [\" wo\", \"rld\", \"he\", \"llo\", \" world\"])\n"
      ],
      "metadata": {
        "id": "8cIGWsLQyX-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1821c6cc-600a-47db-ee9a-110f9317bb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'llo', ' world']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note on Optimal Tokenization**:\n",
        "> As the vocabulary grows, it's important to prefer longer tokens where you have a choice, for example:\n",
        "> ```python\n",
        "> >>> tokenize(\"hello world\", [\" wo\", \"rld\", \"he\", \"llo\", \" world\"])\n",
        "> ['he', 'llo', ' world']\n",
        "> ```\n",
        "> If your initial implementation does not account for this you can revisit before proceeding.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXi4bKkC6S3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Vocabulary Construction\n",
        "The second task is to derive a vocabulary of a specific size based on the provided corpus. The algorithm can be broken down into the following steps:\n",
        "\n",
        "1. Operate on the chunks defined below as a corpus.\n",
        "2. Assemble a character-level vocabulary—i.e., a vocabulary of tokens in which all tokens are of length 1—based on all characters that appear throughout the text chunks.\n",
        "3. Merge the pair of tokens that occurs most frequently throughout the chunks into a new, longer token.\n",
        "\n",
        "    a. First, implement a `to_pairs` function in terms of tokenize that accepts a text chunk and a vocabulary and returns the list of pairs that occur in the chunk\n",
        "\n",
        "    b. Then, implement a `next_most_frequent_pair` function that accepts the full list of text chunks and the current vocabulary to determine the next pair of tokens that should be merged\n",
        "\n",
        "\n",
        "Step 3 can be repeated until the desired vocabulary size is reached. For this exercise, build a vocabulary of size 256.\n"
      ],
      "metadata": {
        "id": "Vi7dNm6WyFkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install html2text"
      ],
      "metadata": {
        "id": "nZV-MQmDxeza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7ca17a-99cd-4117-fac7-35194c7024e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html2text\n",
            "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2025.4.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLbTY8VCxpPK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import html2text\n",
        "h = html2text.HTML2Text()\n",
        "h.ignore_links = True\n",
        "h.ignore_images = True\n",
        "\n",
        "# Load full text of document and split into chunks\n",
        "full_text = h.handle(requests.get('https://avalon.law.yale.edu/18th_century/ny01.asp').text)\n",
        "chunks = full_text.split('\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_vocab(text):\n",
        "  # get all the unique characters from the text\n",
        "  base_vocab = list(set(text))\n",
        "  print(len(base_vocab))\n",
        "  return base_vocab\n",
        "base_vocab = get_base_vocab(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjaQHqTf4Kay",
        "outputId": "d075dd51-b1f2-4a69-9137-c00ecd2e3adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pairs(chunk: str, vocab):\n",
        "  pairs = []\n",
        "  tokens = tokenize(chunk, vocab)\n",
        "  for i in range(len(tokens) - 1):\n",
        "    pairs.append((tokens[i], tokens[i + 1]))\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "U5r-FaTCyTbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(to_pairs(chunks[0], base_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9kP0i_418B",
        "outputId": "0817ce1b-3f7a-4309-d44b-905472d89e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(' ', ' '), (' ', '*'), ('*', ' '), (' ', '|'), ('|', ' '), (' ', ' '), (' ', ' '), (' ', '\\n'), ('\\n', '-'), ('-', '-'), ('-', '-'), ('-', '|'), ('|', '-'), ('-', '-'), ('-', '-'), ('-', ' '), (' ', ' ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def next_most_frequent_pair(chunks, vocab):\n",
        "  pairs_count = {}\n",
        "  for chunk in chunks:\n",
        "    pairs = to_pairs(chunk, vocab)\n",
        "    for pair in pairs:\n",
        "      pairs_count[pair] = pairs_count.get(pair, 0) + 1\n",
        "  return max(pairs_count, key = pairs_count.get)"
      ],
      "metadata": {
        "id": "GCooxERyydJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_most_frequent_pair(chunks, base_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8HesFou6G4M",
        "outputId": "0da8bce3-efb4-4a68-bbe2-c2e475cb7be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', ' ')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def build_vocab(chunks, target_size = 256):\n",
        "  vocab = base_vocab\n",
        "  while (len(vocab) < target_size):\n",
        "    most_frequent_pair = next_most_frequent_pair(chunks, vocab)\n",
        "    if most_frequent_pair is None:\n",
        "      break\n",
        "    new_token = most_frequent_pair[0] + most_frequent_pair[1]\n",
        "    vocab.append(new_token)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "DhcsVlNZ7iyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(build_vocab(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w3GFo3z8jWV",
        "outputId": "54e4d1f3-ab8f-4fed-a172-af8dcb49317d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', '0', '2', 'O', 'C', 'V', 'n', '|', '3', '9', \"'\", 'u', '(', 'Â', '£', '8', 'i', 'r', 'b', '#', 'l', ')', 'k', '4', 'E', '_', 'g', 'A', 'X', '6', 'F', 'a', 'j', '~', ',', 'N', ';', 'P', 'U', 'Y', 'h', 'M', 's', 'W', ':', '.', 'R', 'p', '-', 'T', 'H', '>', '*', 'K', '\"', 'I', 'v', 't', 'L', 'o', '5', 'f', 'q', ' ', '1', 'x', 'w', 'e', 'm', 'd', 'J', 'G', 'Q', 'D', 'y', 'z', 'c', '\\n', '7', 'B', 'e ', 'th', ' th', ' a', ' o', 'er', 'in', ' the ', 'd ', 'on', 'at', 'en', 'es', ' of', ', ', 'or', 'an', 's ', 'al', 'ti', 'to', 're', 'of', 'ed ', 't ', 'y ', 'and ', 'all', 'sh', 'the ', 'ou', 'ec', 'ar', ' b', 'is', 'shall', 'ch', 'e, ', 'con', 'it', 'for', 'ver', ' an', 'is ', 'of ', 'tat', '. ', 'e\\n', 'su', 'res', 'ol', 'tion', 'to ', 's, ', 'el', 'ing', 'ic', 'em', 'wh', 'as ', 'ed', 'of the ', 'ro', 'Stat', 'as', 'r ', 'by ', 'ent ', 'ion', 'go', 'ter', 'sa', 'at ', 'ther', 'cou', 'ur', 'e of ', 'Th', 'pe', 'in ', 'bl', 'ent', 'be ', 'ac', 'un', ' and ', 'this ', 'il', 'ith', 'id', 'po', 'pl', ' be ', 'vern', ' of the ', 'govern', 'ei', ' of ', ', and ', 'eg', 'or ', 'that ', 'om', 'iv', 'ted ', 'ation', 'ain', 'said', 'in the ', 'tr', 'ex', 'with', 'coun', 'shall ', 'shall be ', '  ', '--', 'cons', 'this Stat', 'a ', 'lat', 'ha', ' to ', 'nd ', 'pro', 'eir ', 'me ', 'pres', ' d', 'by the ', 'd\\n', 'leg', 'ass', 'sen', 'tin', 'ever', 'ber', 'y of ', 've ', 'legis', 'legislat', 'ts ', 'us', 'w ', 'no', 'such', 'peo', 'peopl', 'the\\n', '\\nthe ', 'ere', 'ut', 'de', 'That ', 'ri', 'And ', 'fic', 'their ', 'ma', 'legislatur', ',\\n', 'said ', 'cl', 'li', 'gh', 'es ', 'min', 'senat', 'ir', 'ap', 'elec', 'ce ', 'y, ', 'ment', 'ta', 'ich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMT-xV-28mbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}